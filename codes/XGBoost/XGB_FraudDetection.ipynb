{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 01 - EDA with Pyspark\n",
    "\n",
    "Gradient Boosted Trees applied to Fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FraudTreeMethods').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "#data = sqlContext.sql(\"SELECT * FROM fraud_train_sample_csv\")\n",
    "data = spark.read.csv('train_sample.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the click time to day and hour and add it to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+----+---+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|hour|day|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+----+---+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|   9|  7|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|  13|  7|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|  18|  7|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|   4|  7|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|   9|  9|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, minute, dayofmonth\n",
    "data = data.withColumn('hour',hour(data.click_time)).\\\n",
    "             withColumn('day',dayofmonth(data.click_time))\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feathering\n",
    "\n",
    "Feathering, grouping-merging as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = data.select(\"ip\",\"day\",\"hour\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"day\",\"hour\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_day_hour_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "data = data.join(gp, [\"ip\",\"day\",\"hour\"])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].\\\n",
    "            count().reset_index().\\\n",
    "            rename(index=str, columns={'channel': '*ip_app_count_channel'})\n",
    "df = df.merge(gp, on=['ip','app'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = data.select(\"ip\",\"app\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"app\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "data = data.join(gp, [\"ip\",\"app\"])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','app', 'os', 'channel']].\\\n",
    "            groupby(by=['ip', 'app', 'os'])[['channel']].\\\n",
    "            count().reset_index().\\\n",
    "            rename(index=str, columns={'channel': '*ip_app_os_count_channel'})\n",
    "df = df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = data.select('ip','app', 'os', 'channel')\\\n",
    "               .groupBy('ip', 'app', 'os')\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_os_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "data = data.join(gp, ['ip','app', 'os'])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','day','hour','channel']].\\\n",
    "            groupby(by=['ip','day','channel'])[['hour']].\\\n",
    "            var().reset_index().\\\n",
    "            rename(index=str, columns={'hour': '*ip_tchan_count'})\n",
    "df = df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = data.select('ip','day','hour','channel')\\\n",
    "               .groupBy('ip','day','channel')\\\n",
    "               .agg({\"hour\":\"variance\"})\\\n",
    "               .withColumnRenamed(\"variance(hour)\", \"*ip_day_chan_var_hour\")\\\n",
    "               .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+---------------------+\n",
      "|summary|               ip|               day|           channel|*ip_day_chan_var_hour|\n",
      "+-------+-----------------+------------------+------------------+---------------------+\n",
      "|  count|            93176|             93176|             93176|                93176|\n",
      "|   mean|92922.74640465356| 7.858386279728686| 270.4198828024384|                  NaN|\n",
      "| stddev|  70800.947593547|0.8932649462156179|130.69830554337338|                  NaN|\n",
      "|    min|                9|                 6|                 3|                  0.0|\n",
      "|    max|           364757|                 9|               498|                  NaN|\n",
      "+-------+-----------------+------------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gp.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the number of nan and null in the gp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------------------+\n",
      "| ip|day|channel|*ip_day_chan_var_hour|\n",
      "+---+---+-------+---------------------+\n",
      "|  0|  0|      0|                89123|\n",
      "+---+---+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "gp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in gp.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remeber from python EDA the following \n",
    "\n",
    "```python\n",
    "ip                                0\n",
    "app                               0\n",
    "device                            0\n",
    "os                                0\n",
    "channel                           0\n",
    "click_time                        0\n",
    "is_attributed                     0\n",
    "hour                              0\n",
    "day                               0\n",
    "*ip_day_hour_count_channel        0\n",
    "*ip_app_count_channel             0\n",
    "*ip_app_os_count_channel          0\n",
    "*ip_tchan_count               89123\n",
    "*ip_app_os_var                89715\n",
    "*ip_app_channel_var_day       84834\n",
    "*ip_app_channel_mean_hour         0\n",
    "dtype: int64\n",
    "\n",
    "```\n",
    "Therefore we skip the following grouping (columns)as follow.\n",
    "\n",
    "```python\n",
    "*ip_tchan_count               10877 non-null float64\n",
    "*ip_app_os_var                10285 non-null float64\n",
    "*ip_app_channel_var_day       15166 non-null float64\n",
    "\n",
    "```\n",
    "Note that the last gp was not joined into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','app', 'channel','hour']].\\\n",
    "            groupby(by=['ip', 'app', 'channel'])[['hour']].\\\n",
    "            mean().reset_index().\\\n",
    "            rename(index=str, columns={'hour': '*ip_app_channel_mean_hour'})\n",
    "\n",
    "df = df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = data.select('ip','app', 'channel','hour')\\\n",
    "               .groupBy('ip', 'app', 'channel')\\\n",
    "               .agg({\"hour\":\"mean\"})\\\n",
    "               .withColumnRenamed(\"mean(hour)\", \"*ip_app_channel_mean_hour\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "data = data.join(gp, ['ip', 'app', 'channel'])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- click_time: string (nullable = true)\n",
      " |-- attributed_time: string (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n",
      " |-- *ip_day_hour_count_channel: long (nullable = false)\n",
      " |-- *ip_app_count_channel: long (nullable = false)\n",
      " |-- *ip_app_os_count_channel: long (nullable = false)\n",
      " |-- avg(hour): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|               ip|              app|           channel|                os|               day|             hour|\n",
      "+-------+-----------------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|           100000|           100000|            100000|            100000|            100000|           100000|\n",
      "|   mean|      91255.87967|         12.04788|         268.83246|          22.81828|           7.86146|          9.32859|\n",
      "| stddev|69835.55366125253|14.94149992436502|129.72424821194426|55.943135898751194|0.8892663135111792|6.180585781971322|\n",
      "|    min|                9|                1|                 3|                 0|                 6|                0|\n",
      "|    max|           364757|              551|               498|               866|                 9|               23|\n",
      "+-------+-----------------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data.summary().show()\n",
    "cols1 = ['ip', 'app', 'channel',\n",
    "       'os', 'day', 'hour']\n",
    "data.describe(cols1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+-------------------+--------------------+\n",
      "|summary|            device|         click_time|    attributed_time|       is_attributed|\n",
      "+-------+------------------+-------------------+-------------------+--------------------+\n",
      "|  count|            100000|             100000|                227|              100000|\n",
      "|   mean|          21.77125|               null|               null|             0.00227|\n",
      "| stddev|259.66776742008614|               null|               null|0.047590647702016924|\n",
      "|    min|                 0|2017-11-06 16:00:00|2017-11-06 17:19:04|                   0|\n",
      "|    max|              3867|2017-11-09 15:59:51|2017-11-09 15:28:15|                   1|\n",
      "+-------+------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols2 = ['device', 'click_time', \n",
    "        'attributed_time','is_attributed']\n",
    "data.describe(cols2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------+---------------------+------------------------+\n",
      "|summary|*ip_day_hour_count_channel|*ip_app_count_channel|*ip_app_os_count_channel|\n",
      "+-------+--------------------------+---------------------+------------------------+\n",
      "|  count|                    100000|               100000|                  100000|\n",
      "|   mean|                   1.49328|              3.58026|                 1.29488|\n",
      "| stddev|        2.0205929005014096|   10.553763885539677|      1.6443882831400434|\n",
      "|    min|                         1|                    1|                       1|\n",
      "|    max|                        28|                  132|                      33|\n",
      "+-------+--------------------------+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols3 = ['*ip_day_hour_count_channel',\n",
    "       '*ip_app_count_channel',\n",
    "       '*ip_app_os_count_channel']\n",
    "data.describe(cols3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the uniques number for each column in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+---+---+----+------+----------+---------------+-------------+--------------------------+---------------------+------------------------+---------+\n",
      "|   ip|app|channel| os|day|hour|device|click_time|attributed_time|is_attributed|*ip_day_hour_count_channel|*ip_app_count_channel|*ip_app_os_count_channel|avg(hour)|\n",
      "+-----+---+-------+---+---+----+------+----------+---------------+-------------+--------------------------+---------------------+------------------------+---------+\n",
      "|34857|161|    161|130|  4|  24|   100|     80350|            227|            2|                        27|                   58|                      23|      324|\n",
      "+-----+---+-------+---+---+----+------+----------+---------------+-------------+--------------------------+---------------------+------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "data.agg(*(countDistinct(col(c)).alias(c) for c in data.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 439\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|\n",
      "| 93663|  3|     1| 17|    115|2017-11-09 01:22:13|           null|            0|\n",
      "| 17059|  1|     1| 17|    135|2017-11-09 01:17:58|           null|            0|\n",
      "|121505|  9|     1| 25|    442|2017-11-07 10:01:53|           null|            0|\n",
      "|192967|  2|     2| 22|    364|2017-11-08 09:35:17|           null|            0|\n",
      "|143636|  3|     1| 19|    135|2017-11-08 12:35:26|           null|            0|\n",
      "| 73839|  3|     1| 22|    489|2017-11-08 08:14:37|           null|            0|\n",
      "| 34812|  3|     1| 13|    489|2017-11-07 05:03:14|           null|            0|\n",
      "|114809|  3|     1| 22|    205|2017-11-09 10:24:23|           null|            0|\n",
      "|114220|  6|     1| 20|    125|2017-11-08 14:46:16|           null|            0|\n",
      "| 36150|  2|     1| 13|    205|2017-11-07 00:54:09|           null|            0|\n",
      "| 72116| 25|     2| 19|    259|2017-11-08 23:17:45|           null|            0|\n",
      "|  5314|  2|     1|  2|    477|2017-11-09 07:33:41|           null|            0|\n",
      "|106598|  3|     1| 20|    280|2017-11-09 03:44:35|           null|            0|\n",
      "| 72065| 20|     2| 90|    259|2017-11-06 23:14:08|           null|            0|\n",
      "| 37301| 14|     1| 13|    349|2017-11-06 20:07:00|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# over sampling\n",
    "major_df = data.filter(col(\"is_attributed\") == 0)\n",
    "minor_df = data.filter(col(\"is_attributed\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))\n",
    "a = range(ratio)\n",
    "\n",
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "\n",
    "# combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "combined_df.show()\n",
    "data = combined_df\n",
    "data = data.drop('click_time','attributed_time')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "assembler = VectorAssembler(inputCols=['ip', 'app', 'device', 'os', 'channel'],outputCol=\"features\")\n",
    "trainingData = assembler.transform(trainingData)\n",
    "testData = assembler.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+\n",
      "|prediction|is_attributed|            features|\n",
      "+----------+-------------+--------------------+\n",
      "|       0.0|            0|[10.0,12.0,1.0,19...|\n",
      "|       0.0|            0|[20.0,2.0,1.0,9.0...|\n",
      "|       0.0|            0|[20.0,2.0,1.0,16....|\n",
      "|       0.0|            0|[20.0,12.0,1.0,13...|\n",
      "|       0.0|            0|[20.0,18.0,1.0,19...|\n",
      "+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", maxIter=20, maxDepth=4)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = gbt.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"is_attributed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0536021\n",
      "Test accuracy = 0.946398\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_attributed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Test accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|31620|\n",
      "|       1.0|28359|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to test, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|click_id|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       1|       0.0|\n",
      "|       2|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+-------------+\n",
      "|click_id|is_attributed|\n",
      "+--------+-------------+\n",
      "|       0|          1.0|\n",
      "|       1|          0.0|\n",
      "|       2|          0.0|\n",
      "+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------+--------+\n",
      "|is_attributed|   count|\n",
      "+-------------+--------+\n",
      "|          0.0|18152789|\n",
      "|          1.0|  637680|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv('test.csv', inferSchema=True, header=True)\n",
    "#test.show(5)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['ip', 'app', 'device', 'os', 'channel'],outputCol=\"features\")\n",
    "test = assembler.transform(test)\n",
    "#test.show(3)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "#predictions.show(2)\n",
    "\n",
    "data_to_submit = predictions.select(['click_id','prediction'])\n",
    "data_to_submit.show(3)\n",
    "\n",
    "data_to_submit = data_to_submit.withColumnRenamed('prediction','is_attributed')\n",
    "data_to_submit.show(3)\n",
    "\n",
    "data_to_submit.groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is runing now\n"
     ]
    }
   ],
   "source": [
    "print('it is runing now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "FraudDetection_RF_XGB",
  "notebookId": 404476875724424
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
