{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and Gradient Boosted Trees used for Fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T04:43:46.230438600Z",
     "start_time": "2023-09-26T04:43:46.143526Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomForestClassifier\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MulticlassClassificationEvaluator\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FraudTreeMethods').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 83230|  3|     1| 13|    379|2017-11-06 14:32:21|           null|            0|\n",
      "| 17357|  3|     1| 19|    379|2017-11-06 14:33:34|           null|            0|\n",
      "| 35810|  3|     1| 13|    379|2017-11-06 14:34:12|           null|            0|\n",
      "| 45745| 14|     1| 13|    478|2017-11-06 14:34:52|           null|            0|\n",
      "|161007|  3|     1| 13|    379|2017-11-06 14:35:08|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "#data = sqlContext.sql(\"SELECT * FROM fraud_train_sample\")\n",
    "data = spark.read.csv('train.csv', inferSchema=True, header=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+----------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+----------+---------------+-------------+\n",
      "|277396|706|  3475|800|    202|    259620|         182057|            2|\n",
      "+------+---+------+---+-------+----------+---------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "data.agg(*(countDistinct(col(c)).alias(c) for c in data.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|is_attributed|    count|\n",
      "+-------------+---------+\n",
      "|            1|   456846|\n",
      "|            0|184447044|\n",
      "+-------------+---------+\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over sampling\n",
    "\n",
    "### The imbalance ratio of 0 and 1 are calculated at the following and it is used for oversampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 403\n"
     ]
    }
   ],
   "source": [
    "major_df = data.filter(col(\"is_attributed\") == 0)\n",
    "minor_df = data.filter(col(\"is_attributed\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ratio is applied for oversampling at the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|   ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|83230|  3|     1| 13|    379|2017-11-06 14:32:21|           null|            0|\n",
      "|17357|  3|     1| 19|    379|2017-11-06 14:33:34|           null|            0|\n",
      "|35810|  3|     1| 13|    379|2017-11-06 14:34:12|           null|            0|\n",
      "+-----+---+------+---+-------+-------------------+---------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "a = range(ratio)\n",
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "# combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "combined_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data is combined at the following and is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|is_attributed|    count|\n",
      "+-------------+---------+\n",
      "|            1|184108938|\n",
      "|            0|184447044|\n",
      "+-------------+---------+\n"
     ]
    }
   ],
   "source": [
    "combined_df.groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the new data (combined_df) into the old data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|is_attributed|    count|\n",
      "+-------------+---------+\n",
      "|            1|184108938|\n",
      "|            0|184447044|\n",
      "+-------------+---------+\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+----------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+----------+---------------+-------------+\n",
      "|277396|706|  3475|800|    202|    259620|         182057|            2|\n",
      "+------+---+------+---+-------+----------+---------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "data.agg(*(countDistinct(col(c)).alias(c) for c in data.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop two columns including the click time and attributed time. This is done for sack of simplicity in this toturial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---+-------+-------------+\n",
      "|   ip|app|device| os|channel|is_attributed|\n",
      "+-----+---+------+---+-------+-------------+\n",
      "|83230|  3|     1| 13|    379|            0|\n",
      "|17357|  3|     1| 19|    379|            0|\n",
      "|35810|  3|     1| 13|    379|            0|\n",
      "+-----+---+------+---+-------+-------------+\n"
     ]
    }
   ],
   "source": [
    "data = data.drop('click_time','attributed_time')\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ip=83230, app=3, device=1, os=13, channel=379, is_attributed=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+---+-------+-------------+\n",
      "| ip|app|device| os|channel|is_attributed|\n",
      "+---+---+------+---+-------+-------------+\n",
      "|  9|  3|     1| 13|    115|            0|\n",
      "|  9|  3|     1| 13|    135|            0|\n",
      "|  9|  3|     1| 13|    280|            0|\n",
      "+---+---+------+---+-------+-------------+\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip', 'app', 'device', 'os', 'channel', 'is_attributed']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data before feeding into the ML algorithm. Determine the columns name as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['ip', 'app', 'device', 'os', 'channel'],outputCol=\"features\")\n",
    "trainingData = assembler.transform(trainingData)\n",
    "testData = assembler.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+---+-------+-------------+--------------------+\n",
      "| ip|app|device| os|channel|is_attributed|            features|\n",
      "+---+---+------+---+-------+-------------+--------------------+\n",
      "|  9|  3|     1| 13|    115|            0|[9.0,3.0,1.0,13.0...|\n",
      "|  9|  3|     1| 13|    135|            0|[9.0,3.0,1.0,13.0...|\n",
      "|  9|  3|     1| 13|    280|            0|[9.0,3.0,1.0,13.0...|\n",
      "+---+---+------+---+-------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asign the model, train the model, and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification\n",
    "# Train a RandomForest model.\n",
    "#rf = RandomForestClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", numTrees=20)\n",
    "rf = RandomForestClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", numTrees=30, maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+\n",
      "|prediction|is_attributed|            features|\n",
      "+----------+-------------+--------------------+\n",
      "|       0.0|            0|[9.0,2.0,1.0,19.0...|\n",
      "|       0.0|            0|[9.0,3.0,1.0,13.0...|\n",
      "|       0.0|            0|[9.0,3.0,1.0,13.0...|\n",
      "|       0.0|            0|[9.0,3.0,1.0,13.0...|\n",
      "|       0.0|            0|[9.0,9.0,1.0,13.0...|\n",
      "+----------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"is_attributed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_attributed\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.154955\n",
      "Test accuracy = 0.845045\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Test accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.1632, 1: 0.6297, 2: 0.0535, 3: 0.0019, 4: 0.1517})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a very good example to show this!\n",
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to test dataset and predict.\n",
    "#### Read the test data first and prepare the format so it can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+------+---+-------+-------------------+\n",
      "|click_id|    ip|app|device| os|channel|         click_time|\n",
      "+--------+------+---+------+---+-------+-------------------+\n",
      "|       0|  5744|  9|     1|  3|    107|2017-11-10 04:00:00|\n",
      "|       1|119901|  9|     1|  3|    466|2017-11-10 04:00:00|\n",
      "|       2| 72287| 21|     1| 19|    128|2017-11-10 04:00:00|\n",
      "|       3| 78477| 15|     1| 13|    111|2017-11-10 04:00:00|\n",
      "|       4|123080| 12|     1| 13|    328|2017-11-10 04:00:00|\n",
      "+--------+------+---+------+---+-------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('test.csv', inferSchema=True, header=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+------+---+-------+-------------------+\n",
      "|click_id|    ip|app|device| os|channel|         click_time|\n",
      "+--------+------+---+------+---+-------+-------------------+\n",
      "|       0|  5744|  9|     1|  3|    107|2017-11-10 04:00:00|\n",
      "|       1|119901|  9|     1|  3|    466|2017-11-10 04:00:00|\n",
      "|       2| 72287| 21|     1| 19|    128|2017-11-10 04:00:00|\n",
      "+--------+------+---+------+---+-------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "#data = data.drop('click_time','attributed_time')\n",
    "test = data\n",
    "#test = data.select(['ip','app','device','os','channel'])\n",
    "test.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecorize the test data and transfer it as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['ip', 'app', 'device', 'os', 'channel'],outputCol=\"features\")\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+------+---+-------+-------------------+--------------------+\n",
      "|click_id|    ip|app|device| os|channel|         click_time|            features|\n",
      "+--------+------+---+------+---+-------+-------------------+--------------------+\n",
      "|       0|  5744|  9|     1|  3|    107|2017-11-10 04:00:00|[5744.0,9.0,1.0,3...|\n",
      "|       1|119901|  9|     1|  3|    466|2017-11-10 04:00:00|[119901.0,9.0,1.0...|\n",
      "|       2| 72287| 21|     1| 19|    128|2017-11-10 04:00:00|[72287.0,21.0,1.0...|\n",
      "+--------+------+---+------+---+-------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "test.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fed the test data set into the model and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+------+---+-------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|click_id|    ip|app|device| os|channel|         click_time|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+------+---+------+---+-------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|  5744|  9|     1|  3|    107|2017-11-10 04:00:00|[5744.0,9.0,1.0,3...|[20.6742278268919...|[0.68914092756306...|       0.0|\n",
      "|       1|119901|  9|     1|  3|    466|2017-11-10 04:00:00|[119901.0,9.0,1.0...|[24.8633981858914...|[0.82877993952971...|       0.0|\n",
      "+--------+------+---+------+---+-------+-------------------+--------------------+--------------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "predictions.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a submiting file as follow and determine the columns name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|click_id|prediction|\n",
      "+--------+----------+\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       2|       1.0|\n",
      "+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "data_to_submit = predictions.select(['click_id','prediction'])\n",
    "data_to_submit.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|click_id|is_attributed|\n",
      "+--------+-------------+\n",
      "|       0|          0.0|\n",
      "|       1|          0.0|\n",
      "|       2|          1.0|\n",
      "+--------+-------------+\n"
     ]
    }
   ],
   "source": [
    "data_to_submit = data_to_submit.withColumnRenamed('prediction','is_attributed')\n",
    "data_to_submit.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets before submiting the results, look inside and see the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|is_attributed|   count|\n",
      "+-------------+--------+\n",
      "|          0.0|15921222|\n",
      "|          1.0| 2869247|\n",
      "+-------------+--------+\n"
     ]
    }
   ],
   "source": [
    "data_to_submit.groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not very balance results. We need to improve the results by tuning the model hyperparameters plus some data feathering are suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the output file and submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_to_submit.to_csv('RFmodeling.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_to_submit.coalesce(1).write.option(\"header\",\"true\").option(\"sep\",\",\").mode(\"overwrite\").csv(\"output/path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "FraudDetection_RF_XGB",
  "notebookId": 404476875724424
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
