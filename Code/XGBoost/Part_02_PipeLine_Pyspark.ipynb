{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 02 - Pyspark\n",
    "\n",
    "Gradient Boosted Trees applied to Fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import pow, col\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import col, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Time_converter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FraudTreeMethods').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting the parent directory into current path\n",
    "sys.path.insert(1, '../work/data_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'train_sample.csv'\n",
    "dataset_address = '../work/data_set/'\n",
    "path = dataset_address + data_name\n",
    "RDD = spark.read.csv(path, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RDD.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD.printSchema is \n",
      "\n",
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- click_time: string (nullable = true)\n",
      " |-- attributed_time: string (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RDD.printSchema is \\n')\n",
    "RDD.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time_function(RDD):\n",
    "    \n",
    "    from pyspark.sql.functions import hour, minute, dayofmonth\n",
    "    RDD = RDD.withColumn('hour',hour(RDD.click_time)).\\\n",
    "                 withColumn('day',dayofmonth(RDD.click_time))\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_function(RDD):\n",
    "\n",
    "    gp = RDD.select(\"ip\",\"day\",\"hour\", \"channel\")\\\n",
    "                   .groupBy(\"ip\",\"day\",\"hour\")\\\n",
    "                   .agg({\"channel\":\"count\"})\\\n",
    "                   .withColumnRenamed(\"count(channel)\", \"*ip_day_hour_count_channel\")\\\n",
    "                   .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, [\"ip\",\"day\",\"hour\"])\\\n",
    "             .sort(col(\"ip\"))\n",
    "    \n",
    "    \n",
    "    gp = RDD.select(\"ip\",\"app\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"app\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, [\"ip\",\"app\"])\\\n",
    "             .sort(col(\"ip\"))\n",
    "    \n",
    "    \n",
    "    gp = RDD.select('ip','app', 'os', 'channel')\\\n",
    "               .groupBy('ip', 'app', 'os')\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_os_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, ['ip','app', 'os'])\\\n",
    "             .sort(col(\"ip\"))\n",
    "    \n",
    "    \n",
    "    gp = RDD.select('ip','app', 'channel','hour')\\\n",
    "               .groupBy('ip', 'app', 'channel')\\\n",
    "               .agg({\"hour\":\"mean\"})\\\n",
    "               .withColumnRenamed(\"avg(hour)\", \"*ip_app_channel_mean_hour\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, ['ip', 'app', 'channel'])\\\n",
    "             .sort(col(\"ip\"))\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_functions(RDD):\n",
    " \n",
    "    # over sampling\n",
    "    major_df = RDD.filter(col(\"is_attributed\") == 0)\n",
    "    minor_df = RDD.filter(col(\"is_attributed\") == 1)\n",
    "    ratio = int(major_df.count()/minor_df.count())\n",
    "#     print(\"ratio: {}\".format(ratio))\n",
    "    a = range(ratio)\n",
    "    \n",
    "    # duplicate the minority rows\n",
    "    oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "    \n",
    "    # combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
    "    RDD = major_df.unionAll(oversampled_df)\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfering_functions(RDD):\n",
    "\n",
    "    def transformer(x):\n",
    "        x = pow(x, (0.05))\n",
    "        return x\n",
    "    \n",
    "    RDD = RDD.withColumn(\"app\", transformer('app'))\n",
    "    RDD = RDD.withColumn(\"device\", transformer('device'))\n",
    "    RDD = RDD.withColumn(\"os\", transformer('os'))\n",
    "    RDD = RDD.withColumn(\"day\", transformer('day'))\n",
    "\n",
    "    RDD = RDD.withColumn(\"*ip_day_hour_count_channel\", transformer('*ip_day_hour_count_channel'))\n",
    "\n",
    "    RDD = RDD.withColumn(\"*ip_app_count_channel\", transformer('*ip_app_count_channel'))\n",
    "\n",
    "    RDD = RDD.withColumn(\"*ip_app_os_count_channel\", transformer('*ip_app_os_count_channel'))\n",
    "\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(RDD):\n",
    "    \n",
    "    RDD = RDD.drop('click_time','attributed_time')\n",
    "    # Split the data into training and test sets (30% held out for testing)\n",
    "    (trainingData, testData) = RDD.randomSplit([0.7, 0.3])\n",
    "\n",
    "    cols = ['ip',\n",
    "     'app',\n",
    "     'channel',\n",
    "     'os',\n",
    "     'day',\n",
    "     'hour',\n",
    "     'device',\n",
    "     'is_attributed',\n",
    "     '*ip_day_hour_count_channel',\n",
    "     '*ip_app_count_channel',\n",
    "     '*ip_app_os_count_channel',\n",
    "     '*ip_app_channel_mean_hour']\n",
    "\n",
    "    assembler = VectorAssembler(inputCols = cols,outputCol=\"features\")\n",
    "    trainingData = assembler.transform(trainingData)\n",
    "    testData = assembler.transform(testData)\n",
    "    \n",
    "    return assembler, trainingData, testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_XGB_on_data(assembler, trainingData, testData):\n",
    "    # Train a GBT model.\n",
    "    gbt = GBTClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", maxIter=20, maxDepth=4)\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = gbt.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"prediction\", \"is_attributed\", \"features\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictions):\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"is_attributed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    print(\"Test accuracy = %g\" % (accuracy))\n",
    "    predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all functions in a sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_train(RDD):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    RDD = fix_time_function(RDD); print('fix_time_function Done!'); Time_converter.Apply(time.time() - start)\n",
    "    RDD = grouping_function(RDD); print('grouping_function Done!'); Time_converter.Apply(time.time() - start)\n",
    "    RDD = oversampling_functions(RDD); print('oversampling_functions Done!'); Time_converter.Apply(time.time() - start)\n",
    "    RDD = transfering_functions(RDD); print('transfering_functions Done!'); Time_converter.Apply(time.time() - start)\n",
    "    assembler, trainingData, testData = prepare_data_train(RDD); print('prepare_data_train Done!'); \n",
    "    Time_converter.Apply(time.time() - start)\n",
    "    predictions = train_XGB_on_data(assembler, trainingData, testData); print('train_XGB_on_data Done!'); \n",
    "    Time_converter.Apply(time.time() - start)\n",
    "    evaluation(predictions); print('evaluation Done!')\n",
    "    \n",
    "    excution_time = (time.time() - start)\n",
    "    Time_converter.Apply(excution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the functions to the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix_time_function Done!\n",
      "exEcution time is =  0:00:00\n",
      "grouping_function Done!\n",
      "exEcution time is =  0:00:00\n",
      "oversampling_functions Done!\n",
      "exEcution time is =  0:00:23\n",
      "transfering_functions Done!\n",
      "exEcution time is =  0:00:23\n",
      "prepare_data_train Done!\n",
      "exEcution time is =  0:00:24\n",
      "train_XGB_on_data Done!\n",
      "exEcution time is =  0:01:32\n",
      "Test Error = 0\n",
      "Test accuracy = 1\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|29947|\n",
      "|       1.0|30240|\n",
      "+----------+-----+\n",
      "\n",
      "evaluation Done!\n",
      "exEcution time is =  0:02:03\n"
     ]
    }
   ],
   "source": [
    "run_for_train(RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the functions to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'train.csv'\n",
    "dataset_address = '../work/data_set/'\n",
    "path = dataset_address + data_name\n",
    "RDD = spark.read.csv(path, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_test(RDD):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    RDD = fix_time_function(RDD); print('fix_time_function Done!'); Time_converter.Apply(time.time() - start)\n",
    "    RDD = grouping_function(RDD); print('grouping_function Done!'); Time_converter.Apply(time.time() - start)\n",
    "    RDD = oversampling_functions(RDD); print('oversampling_functions Done!'); Time_converter.Apply(time.time() - start)\n",
    "    RDD = transfering_functions(RDD); print('transfering_functions Done!'); Time_converter.Apply(time.time() - start)\n",
    "    assembler, trainingData, testData = prepare_data_train(RDD); print('prepare_data_train Done!'); \n",
    "    Time_converter.Apply(time.time() - start)\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    sc.setCheckpointDir('checkpoint')\n",
    "    assembler.checkpoint(); assembler.checkpoint()\n",
    "    trainingData.checkpoint(); trainingData.checkpoint()\n",
    "    testData.checkpoint(); testData.checkpoint()\n",
    "    \n",
    "    predictions = train_XGB_on_data(assembler, trainingData, testData); print('train_XGB_on_data Done!')\n",
    "    excution_time = (time.time() - start)\n",
    "    Time_converter.Apply(excution_time)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix_time_function Done!\n",
      "exEcution time is =  0:00:00\n",
      "grouping_function Done!\n",
      "exEcution time is =  0:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-04e5471610e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_for_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-89eac4ed9c11>\u001b[0m in \u001b[0;36mrun_for_test\u001b[0;34m(RDD)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_time_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fix_time_function Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTime_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouping_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grouping_function Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTime_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moversampling_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'oversampling_functions Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTime_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransfering_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transfering_functions Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTime_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0massembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prepare_data_train Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9212d4fe86f8>\u001b[0m in \u001b[0;36moversampling_functions\u001b[0;34m(RDD)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmajor_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_attributed\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mminor_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_attributed\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mminor_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     print(\"ratio: {}\".format(ratio))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = run_for_test(RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4d1c142e95ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# home = str(Path.home())\n",
    "# data = [\n",
    "#     (\"jellyfish\", \"JALYF\"),\n",
    "#     (\"li\", \"L\"),\n",
    "#     (\"luisa\", \"LAS\"),\n",
    "#     (None, None)\n",
    "# ]\n",
    "# df = spark.createDataFrame(data, [\"word\", \"expected\"])\n",
    "# df.toPandas().to_csv(home + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(fix_time_function, \n",
    "#                             grouping_function, \n",
    "#                             oversampling_functions, \n",
    "#                             transfering_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD = pipeline.fit(RDD)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "name": "FraudDetection_RF_XGB",
  "notebookId": 404476875724424
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
