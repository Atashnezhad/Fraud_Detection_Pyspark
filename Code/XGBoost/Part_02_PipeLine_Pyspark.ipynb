{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 02 - Pyspark\n",
    "\n",
    "Gradient Boosted Trees applied to Fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import pow, col\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import col, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FraudTreeMethods').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = spark.read.csv('train_sample.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RDD.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD.printSchema is \n",
      "\n",
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- click_time: string (nullable = true)\n",
      " |-- attributed_time: string (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RDD.printSchema is \\n')\n",
    "RDD.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time_function(RDD):\n",
    "    \n",
    "    from pyspark.sql.functions import hour, minute, dayofmonth\n",
    "    RDD = RDD.withColumn('hour',hour(RDD.click_time)).\\\n",
    "                 withColumn('day',dayofmonth(RDD.click_time))\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_function(RDD):\n",
    "\n",
    "    gp = RDD.select(\"ip\",\"day\",\"hour\", \"channel\")\\\n",
    "                   .groupBy(\"ip\",\"day\",\"hour\")\\\n",
    "                   .agg({\"channel\":\"count\"})\\\n",
    "                   .withColumnRenamed(\"count(channel)\", \"*ip_day_hour_count_channel\")\\\n",
    "                   .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, [\"ip\",\"day\",\"hour\"])\\\n",
    "             .sort(col(\"ip\"))\n",
    "    \n",
    "    \n",
    "    gp = RDD.select(\"ip\",\"app\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"app\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, [\"ip\",\"app\"])\\\n",
    "             .sort(col(\"ip\"))\n",
    "    \n",
    "    \n",
    "    gp = RDD.select('ip','app', 'os', 'channel')\\\n",
    "               .groupBy('ip', 'app', 'os')\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_os_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, ['ip','app', 'os'])\\\n",
    "             .sort(col(\"ip\"))\n",
    "    \n",
    "    \n",
    "    gp = RDD.select('ip','app', 'channel','hour')\\\n",
    "               .groupBy('ip', 'app', 'channel')\\\n",
    "               .agg({\"hour\":\"mean\"})\\\n",
    "               .withColumnRenamed(\"avg(hour)\", \"*ip_app_channel_mean_hour\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "    RDD = RDD.join(gp, ['ip', 'app', 'channel'])\\\n",
    "             .sort(col(\"ip\"))\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_functions(RDD):\n",
    " \n",
    "    # over sampling\n",
    "    major_df = RDD.filter(col(\"is_attributed\") == 0)\n",
    "    minor_df = RDD.filter(col(\"is_attributed\") == 1)\n",
    "    ratio = int(major_df.count()/minor_df.count())\n",
    "#     print(\"ratio: {}\".format(ratio))\n",
    "    a = range(ratio)\n",
    "    \n",
    "    # duplicate the minority rows\n",
    "    oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "    \n",
    "    # combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
    "    RDD = major_df.unionAll(oversampled_df)\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfering_functions(RDD):\n",
    "\n",
    "    def transformer(x):\n",
    "        x = pow(x, (0.05))\n",
    "        return x\n",
    "    \n",
    "    RDD = RDD.withColumn(\"app\", transformer('app'))\n",
    "    RDD = RDD.withColumn(\"device\", transformer('device'))\n",
    "    RDD = RDD.withColumn(\"os\", transformer('os'))\n",
    "    RDD = RDD.withColumn(\"day\", transformer('day'))\n",
    "\n",
    "    RDD = RDD.withColumn(\"*ip_day_hour_count_channel\", transformer('*ip_day_hour_count_channel'))\n",
    "\n",
    "    RDD = RDD.withColumn(\"*ip_app_count_channel\", transformer('*ip_app_count_channel'))\n",
    "\n",
    "    RDD = RDD.withColumn(\"*ip_app_os_count_channel\", transformer('*ip_app_os_count_channel'))\n",
    "\n",
    "\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(RDD):\n",
    "    \n",
    "    RDD = RDD.drop('click_time','attributed_time')\n",
    "    # Split the data into training and test sets (30% held out for testing)\n",
    "    (trainingData, testData) = RDD.randomSplit([0.7, 0.3])\n",
    "\n",
    "    cols = ['ip',\n",
    "     'app',\n",
    "     'channel',\n",
    "     'os',\n",
    "     'day',\n",
    "     'hour',\n",
    "     'device',\n",
    "     'is_attributed',\n",
    "     '*ip_day_hour_count_channel',\n",
    "     '*ip_app_count_channel',\n",
    "     '*ip_app_os_count_channel',\n",
    "     '*ip_app_channel_mean_hour']\n",
    "\n",
    "    assembler = VectorAssembler(inputCols = cols,outputCol=\"features\")\n",
    "    trainingData = assembler.transform(trainingData)\n",
    "    testData = assembler.transform(testData)\n",
    "    \n",
    "    return assembler, trainingData, testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_XGB_on_data(assembler, trainingData, testData):\n",
    "    # Train a GBT model.\n",
    "    gbt = GBTClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", maxIter=20, maxDepth=4)\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = gbt.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"prediction\", \"is_attributed\", \"features\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictions):\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"is_attributed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    print(\"Test accuracy = %g\" % (accuracy))\n",
    "    predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(RDD):\n",
    "    \n",
    "    RDD = fix_time_function(RDD); print('fix_time_function Done!')\n",
    "    RDD = grouping_function(RDD); print('grouping_function Done!')\n",
    "    RDD = oversampling_functions(RDD); print('oversampling_functions Done!')\n",
    "    RDD = transfering_functions(RDD); print('transfering_functions Done!')\n",
    "    assembler, trainingData, testData = prepare_data_train(RDD); print('prepare_data_train Done!')\n",
    "    predictions = train_XGB_on_data(assembler, trainingData, testData); print('train_XGB_on_data Done!')\n",
    "    evaluation(predictions); print('evaluation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix_time_function Done!\n",
      "grouping_function Done!\n",
      "oversampling_functions Done!\n",
      "transfering_functions Done!\n",
      "prepare_data_train Done!\n",
      "train_XGB_on_data Done!\n",
      "Test Error = 0\n",
      "Test accuracy = 1\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|29802|\n",
      "|       1.0|29996|\n",
      "+----------+-----+\n",
      "\n",
      "evaluation Done!\n"
     ]
    }
   ],
   "source": [
    "run(RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = spark.read.csv('train.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(fix_time_function, \n",
    "#                             grouping_function, \n",
    "#                             oversampling_functions, \n",
    "#                             transfering_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD = pipeline.fit(RDD)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "FraudDetection_RF_XGB",
  "notebookId": 404476875724424
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
