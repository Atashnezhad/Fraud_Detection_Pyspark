{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 01 - EDA with Pyspark\n",
    "\n",
    "Gradient Boosted Trees applied to Fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyspark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql.functions import pow, col\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import col, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FraudTreeMethods').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting the parent directory into current path\n",
    "sys.path.insert(1, '../work/data_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'train_sample.csv'\n",
    "dataset_address = '../work/data_set/'\n",
    "path = dataset_address + data_name\n",
    "RDD = spark.read.csv(path, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RDD.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RDD.printSchema is \\n')\n",
    "RDD.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the click time to day and hour and add it to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, minute, dayofmonth\n",
    "RDD = RDD.withColumn('hour',hour(RDD.click_time)).\\\n",
    "             withColumn('day',dayofmonth(RDD.click_time))\n",
    "\n",
    "RDD.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feathering\n",
    "\n",
    "Feathering, grouping-merging as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','day','hour','channel']]\\\n",
    "    .groupby(by=['ip','day','hour'])[['channel']]\\\n",
    "    .count().reset_index()\\\n",
    "    .rename(index=str, columns={'channel': '*ip_day_hour_count_channel'})\n",
    "df = df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = RDD.select(\"ip\",\"day\",\"hour\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"day\",\"hour\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_day_hour_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "RDD = RDD.join(gp, [\"ip\",\"day\",\"hour\"])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RDD Columns name = \\n\", RDD.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].\\\n",
    "            count().reset_index().\\\n",
    "            rename(index=str, columns={'channel': '*ip_app_count_channel'})\n",
    "df = df.merge(gp, on=['ip','app'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = RDD.select(\"ip\",\"app\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"app\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "RDD = RDD.join(gp, [\"ip\",\"app\"])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RDD Columns name = \\n\", RDD.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','app', 'os', 'channel']].\\\n",
    "            groupby(by=['ip', 'app', 'os'])[['channel']].\\\n",
    "            count().reset_index().\\\n",
    "            rename(index=str, columns={'channel': '*ip_app_os_count_channel'})\n",
    "df = df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = RDD.select('ip','app', 'os', 'channel')\\\n",
    "               .groupBy('ip', 'app', 'os')\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_os_count_channel\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "RDD = RDD.join(gp, ['ip','app', 'os'])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RDD Columns name = \\n\", RDD.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','day','hour','channel']].\\\n",
    "            groupby(by=['ip','day','channel'])[['hour']].\\\n",
    "            var().reset_index().\\\n",
    "            rename(index=str, columns={'hour': '*ip_day_chan_var_hour'})\n",
    "df = df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = RDD.select('ip','day','hour','channel')\\\n",
    "               .groupBy('ip','day','channel')\\\n",
    "               .agg({\"hour\":\"variance\"})\\\n",
    "               .withColumnRenamed(\"variance(hour)\", \"*ip_day_chan_var_hour\")\\\n",
    "               .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the number of nan and null in the gp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in gp.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remeber from python EDA the following \n",
    "\n",
    "```python\n",
    "ip                                0\n",
    "app                               0\n",
    "device                            0\n",
    "os                                0\n",
    "channel                           0\n",
    "click_time                        0\n",
    "is_attributed                     0\n",
    "hour                              0\n",
    "day                               0\n",
    "*ip_day_hour_count_channel        0\n",
    "*ip_app_count_channel             0\n",
    "*ip_app_os_count_channel          0\n",
    "*ip_tchan_count               89123\n",
    "*ip_app_os_var                89715\n",
    "*ip_app_channel_var_day       84834\n",
    "*ip_app_channel_mean_hour         0\n",
    "dtype: int64\n",
    "\n",
    "```\n",
    "Therefore we skip the following grouping (columns)as follow.\n",
    "\n",
    "```python\n",
    "*ip_tchan_count               10877 non-null float64\n",
    "*ip_app_os_var                10285 non-null float64\n",
    "*ip_app_channel_var_day       15166 non-null float64\n",
    "\n",
    "```\n",
    "Note that the last gp was not joined into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Keep going:**\n",
    "\n",
    "In python EDA we did following:\n",
    "```python\n",
    "gp = df[['ip','app', 'channel','hour']].\\\n",
    "            groupby(by=['ip', 'app', 'channel'])[['hour']].\\\n",
    "            mean().reset_index().\\\n",
    "            rename(index=str, columns={'hour': '*ip_app_channel_mean_hour'})\n",
    "\n",
    "df = df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "\n",
    "```\n",
    "We translate it to Pyspark as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = RDD.select('ip','app', 'channel','hour')\\\n",
    "               .groupBy('ip', 'app', 'channel')\\\n",
    "               .agg({\"hour\":\"mean\"})\\\n",
    "               .withColumnRenamed(\"avg(hour)\", \"*ip_app_channel_mean_hour\")\\\n",
    "               .sort(col(\"ip\"))\n",
    "RDD = RDD.join(gp, ['ip', 'app', 'channel'])\\\n",
    "         .sort(col(\"ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RDD Columns name = \\n\", RDD.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.summary().show()\n",
    "cols1 = ['ip', 'app', 'channel',\n",
    "       'os', 'day', 'hour']\n",
    "RDD.describe(cols1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = ['device', 'click_time', \n",
    "        'attributed_time','is_attributed']\n",
    "RDD.describe(cols2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols3 = ['*ip_day_hour_count_channel',\n",
    "       '*ip_app_count_channel',\n",
    "       '*ip_app_os_count_channel']\n",
    "RDD.describe(cols3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the uniques number for each column in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols4 = cols1 + cols2\n",
    "RDD.agg(*(countDistinct(col(c)).alias(c) for c in cols4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.agg(*(countDistinct(col(c)).alias(c) for c in cols3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over sampling the data\n",
    "\n",
    "* Over sampling\n",
    "* Duplicate the minority rows\n",
    "* Combine both oversampled minority rows and previous majority rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over sampling\n",
    "major_df = RDD.filter(col(\"is_attributed\") == 0)\n",
    "minor_df = RDD.filter(col(\"is_attributed\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))\n",
    "a = range(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both oversampled minority rows and previous majority rows combined_df = major_df.unionAll(oversampled_df)\n",
    "RDD = major_df.unionAll(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RDD Columns name = \\n\", RDD.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn RDD to pandas and use pandas ability for visualization\n",
    "\n",
    "* First take a sample from big RDD\n",
    "* Pass the sample into the pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_RDD = RDD.sample(False, 0.01, 42)\n",
    "data_pd = sub_RDD.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.hist(bins=50, \n",
    "             figsize=(20,15),\n",
    "             facecolor='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.plot(kind=\"scatter\", \n",
    "             x=\"app\", \n",
    "             y=\"channel\", \n",
    "             alpha=0.1, \n",
    "             figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,24))\n",
    "\n",
    "cols = ['app','device','os', \n",
    "        'channel', 'hour', 'day',\n",
    "        '*ip_day_hour_count_channel', '*ip_app_count_channel',\n",
    "        '*ip_app_os_count_channel', '*ip_app_channel_mean_hour']\n",
    "\n",
    "sub_attributed_mask = data_pd[\"is_attributed\"] == 1\n",
    "sub_Not_attributed_mask = data_pd[\"is_attributed\"] == 0\n",
    "\n",
    "\n",
    "for count, col in enumerate(cols, 1):\n",
    "    \n",
    "    plt.subplot(4, 3, count)\n",
    "    plt.hist([data_pd[sub_attributed_mask][col], \n",
    "          data_pd[sub_Not_attributed_mask][col]],\n",
    "          color=['goldenrod', 'grey'],\n",
    "          bins=20, ec='k', density=True)\n",
    "    \n",
    "    plt.title('Count distribution by {}'.format(col), fontsize=12)\n",
    "    plt.legend(['attributed', 'Not_attributed'])\n",
    "    plt.xlabel(col); plt.ylabel('density')\n",
    "\n",
    "# path = '../Figures/'\n",
    "# file_name = 'hist_dens_by_par.png'\n",
    "# plt.savefig(path+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering\n",
    "\n",
    "Applying the transfering achieved from previous EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_colmns = ['app','device','os', 'day', \n",
    "                '*ip_day_hour_count_channel', \n",
    "                '*ip_app_count_channel', \n",
    "                '*ip_app_os_count_channel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(x):\n",
    "    x = pow(x, (0.05))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the defined function into each column as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = RDD.withColumn(\"app\", transformer('app'))\n",
    "RDD = RDD.withColumn(\"device\", transformer('device'))\n",
    "RDD = RDD.withColumn(\"os\", transformer('os'))\n",
    "RDD = RDD.withColumn(\"day\", transformer('day'))\n",
    "\n",
    "RDD = RDD.withColumn(\"*ip_day_hour_count_channel\", transformer('*ip_day_hour_count_channel'))\n",
    "\n",
    "RDD = RDD.withColumn(\"*ip_app_count_channel\", transformer('*ip_app_count_channel'))\n",
    "\n",
    "RDD = RDD.withColumn(\"*ip_app_os_count_channel\", transformer('*ip_app_os_count_channel'))\n",
    "RDD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the click time and attributed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = RDD.drop('click_time','attributed_time')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = RDD.randomSplit([0.7, 0.3])\n",
    "\n",
    "cols = ['ip',\n",
    " 'app',\n",
    " 'channel',\n",
    " 'os',\n",
    " 'day',\n",
    " 'hour',\n",
    " 'device',\n",
    " 'is_attributed',\n",
    " '*ip_day_hour_count_channel',\n",
    " '*ip_app_count_channel',\n",
    " '*ip_app_os_count_channel',\n",
    " '*ip_app_channel_mean_hour']\n",
    "\n",
    "assembler = VectorAssembler(inputCols = cols,outputCol=\"features\")\n",
    "trainingData = assembler.transform(trainingData)\n",
    "testData = assembler.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", maxIter=20, maxDepth=4)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = gbt.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"is_attributed\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"is_attributed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Test accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to test, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'test.csv'\n",
    "dataset_address = '../work/data_set/'\n",
    "path = dataset_address + data_name\n",
    "test = spark.read.csv(path, inferSchema=True, header=True)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the train data schema with the test make sure about dimensions.\n",
    "\n",
    "\n",
    "```python\n",
    "RDD.printSchema is \n",
    "\n",
    "root\n",
    " |-- ip: integer (nullable = true)\n",
    " |-- app: integer (nullable = true)\n",
    " |-- device: integer (nullable = true)\n",
    " |-- os: integer (nullable = true)\n",
    " |-- channel: integer (nullable = true)\n",
    " |-- click_time: string (nullable = true)\n",
    " |-- attributed_time: string (nullable = true)\n",
    " |-- is_attributed: integer (nullable = true)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test.printSchema is \\n')\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, minute, dayofmonth\n",
    "test = test.withColumn('hour',hour(test.click_time)).\\\n",
    "             withColumn('day',dayofmonth(test.click_time))\n",
    "\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply feathering to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = test.select(\"ip\",\"day\",\"hour\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"day\",\"hour\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_day_hour_count_channel\")\n",
    "\n",
    "test = test.join(gp, [\"ip\",\"day\",\"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = test.select(\"ip\",\"app\", \"channel\")\\\n",
    "               .groupBy(\"ip\",\"app\")\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_count_channel\")\n",
    "\n",
    "test = test.join(gp, [\"ip\",\"app\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = test.select('ip','app', 'os', 'channel')\\\n",
    "               .groupBy('ip', 'app', 'os')\\\n",
    "               .agg({\"channel\":\"count\"})\\\n",
    "               .withColumnRenamed(\"count(channel)\", \"*ip_app_os_count_channel\")\n",
    "\n",
    "test = test.join(gp, ['ip','app', 'os'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = test.select('ip','app', 'channel','hour')\\\n",
    "               .groupBy('ip', 'app', 'channel')\\\n",
    "               .agg({\"hour\":\"mean\"})\\\n",
    "               .withColumnRenamed(\"avg(hour)\", \"*ip_app_channel_mean_hour\")\n",
    "\n",
    "test = test.join(gp, ['ip', 'app', 'channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn(\"app\", transformer('app'))\n",
    "test = test.withColumn(\"device\", transformer('device'))\n",
    "test = test.withColumn(\"os\", transformer('os'))\n",
    "test = test.withColumn(\"day\", transformer('day'))\n",
    "\n",
    "test = test.withColumn(\"*ip_day_hour_count_channel\", transformer('*ip_day_hour_count_channel'))\n",
    "\n",
    "test = test.withColumn(\"*ip_app_count_channel\", transformer('*ip_app_count_channel'))\n",
    "\n",
    "test = test.withColumn(\"*ip_app_os_count_channel\", transformer('*ip_app_os_count_channel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = cols,outputCol=\"features\")\n",
    "test = assembler.transform(test)\n",
    "#test.show(3)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "#predictions.show(2)\n",
    "\n",
    "data_to_submit = predictions.select(['click_id','prediction'])\n",
    "data_to_submit.show(3)\n",
    "\n",
    "data_to_submit = data_to_submit.withColumnRenamed('prediction','is_attributed')\n",
    "data_to_submit.show(3)\n",
    "\n",
    "data_to_submit.groupBy('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "FraudDetection_RF_XGB",
  "notebookId": 404476875724424
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
